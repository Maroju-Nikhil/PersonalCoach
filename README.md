# ğŸ§  Pocket Coach  
**Your Personal AI Coach â€” Powered by Local LLMs & Streamlit**

Pocket Coach is a lightweight Progressive Web App (PWA) built with **Streamlit** that integrates with **Ollama** to run local large language models (LLMs) such as **Gemma**, **LLaMA**, or **Mistral**.  
Itâ€™s designed to act as your on-device AI companion â€” accessible on both desktop and mobile (as a standalone PWA).

---

## ğŸš€ Features

- ğŸ’¬ Conversational AI â€” Interact with locally running LLMs for guidance or coaching.  
- âš¡ Local Inference â€” Runs through Ollama, no cloud dependency for model execution.  
- ğŸ“± PWA Support â€” Add it to your mobile home screen and use it like a native app.  
- ğŸ§© Minimalist UI â€” Built with Streamlitâ€™s modern components and styled for dark/black themes.  
- ğŸ§  Modular Design â€” Clean structure with separate files for frontend, backend, and manifest.  

---

## ğŸ“ Project Structure

| File / Folder | Purpose |
|----------------|----------|
| `app.py` | Main entry point for Streamlit. Handles UI rendering, chat logic, and model integration. |
| `static/` | Contains static assets like icons, manifest file, and styles. |
| â”œâ”€â”€ `manifest.json` | Defines PWA behavior (name, theme, icon, standalone mode, etc.). |
| â”œâ”€â”€ `app-icon.png` | PWA icon used when installed on mobile or desktop. |
| `requirements.txt` | Python dependencies for deployment (Streamlit, requests, etc.). |
| `README.md` | Documentation file (this one). |

---

## ğŸ§© How It Works

1. **Frontend (Streamlit)**  
   - Renders the chat interface, message history, and PWA-related assets.  
   - Uses dark/black theme styling for a clean, mobile-friendly interface.  

2. **Backend (Ollama)**  
   - The app sends requests to the Ollama API (running locally on port `11434`).  
   - Ollama handles inference for models like `gemma`, `llama3`, or `mistral`.  

3. **Deployment**  
   - On local systems, Ollama must be running with the selected model pre-loaded.  
   - When deployed to Streamlit Cloud, the model API wonâ€™t connect (as Ollama cannot run remotely).  

4. **PWA Integration**  
   - The `manifest.json` and service worker (served through Streamlit) enable installable PWA functionality.  
   - Once installed, you can use it offline for UI, though AI inference still requires a running Ollama backend.  

---

## ğŸ” System Flow (Architecture Overview)

User â†” Streamlit UI â†” Ollama API â†” Local LLM Model
â”‚
â””â”€â”€ Static Files (manifest.json, icons)


---

## âš™ï¸ Setup & Local Run

1. **Install Ollama** on your machine:  
   https://ollama.ai/download  

2. **Pull your desired model:**  
    >> ollama pull gemma

3. Run Ollama locally:
    >> ollama serve

4. Start the Streamlit app:
    >> streamlit run app.py


Open in your browser:
http://localhost:8501


# â˜ï¸ Deployment & Usage Guide â€” Pocket Coach

## â˜ï¸ Deployment Notes

- **Streamlit Cloud** or **Hugging Face Spaces** currently **cannot host Ollama models**.  
- You can deploy only the **UI (Streamlit app)** to these platforms â€”  
  but it will need a **hosted inference backend** if you want responses from the model.

### ğŸ”„ Alternatives

1. **Host Ollama on a VPS or Local Server**
   - Run Ollama on your own machine or cloud VM.
   - Expose the endpoint securely using:
     - [ngrok](https://ngrok.com)
     - or [Cloudflare Tunnel](https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/)
   - Update your appâ€™s `OLLAMA_API_URL` to point to that endpoint.

2. **Use a Cloud LLM API**
   - Replace Ollama with a hosted API:
     - [OpenAI](https://platform.openai.com)
     - [Groq](https://groq.com)
     - [Anthropic Claude](https://www.anthropic.com)
   - This ensures your app works fully online on Streamlit Cloud.

---

## ğŸ“± Using as a PWA

1. Open the app in your **mobile browser** (e.g., Chrome, Safari).  
2. Tap **â€œAdd to Home Screenâ€** or **â€œInstall App.â€**  
3. Once installed, it appears as a **standalone mobile app** with:
   - Custom icon  
   - Theme color  
   - Fullscreen experience (no browser UI)

âœ… The PWA setup is powered by `manifest.json` and static assets included in the `/static` folder.

---

## ğŸ§¾ Notes

- You can **customize model names**, **themes**, and **chat prompts** directly in `app.py`.  
- The **PWA manifest** is fully configurable:
  - Change app name, short name, and theme colors.
  - Replace the app icon for branding.
- For **true on-device inference**, Pocket Coach depends on **Ollamaâ€™s local runtime**.  

---

## ğŸ§° Tech Stack

| Component | Technology |
|------------|-------------|
| **Frontend** | Streamlit |
| **Backend** | Ollama (Local LLM Engine) |
| **PWA Support** | manifest.json + static asset serving |
| **Language** | Python 3.10+ |

---

## ğŸ§­ Future Improvements

- ğŸ—‚ï¸ Add **persistent chat history** across sessions.  
- ğŸ™ï¸ Integrate **voice input/output** for a conversational feel.  
- â˜ï¸ Provide **remote model fallback** when Ollama is unavailable.  

---

## âš ï¸ Limitations

- âš™ï¸ **Ollama must be running locally** to generate responses.  
- â˜ï¸ **Streamlit Cloud deployment** runs only the UI â€” no local model execution.  
- ğŸ“´ **Offline usage** supports the interface,  
  but AI generation still requires an **active connection to Ollama**.  

---

## ğŸ‘¨â€ğŸ’» Author

**Pocket Coach** was built to bring the power of local LLMs to your pocket â€”  
simple, sleek, and private.  

Contributions and improvements are always welcome! ğŸš€

---


## Deployed link (Although this will not work since model will not get deployed there in streamlit): 
https://personal-coach-of-nikhil.streamlit.app/

